#!/usr/bin/env python3
"""
ğŸ”„ Podcast è‡ªå‹•ç›£æ§å™¨ v4.0

åŠŸèƒ½ï¼š
- æƒæ Whisper output è³‡æ–™å¤¾
- ç™¼ç¾ç¼ºæ‘˜è¦çš„é€å­—ç¨¿è‡ªå‹•ç”Ÿæˆæ‘˜è¦
- è‡ªå‹•æ›´æ–° sidebar.json

ä½¿ç”¨æ–¹æ³•ï¼š
    # è™•ç†æ‰€æœ‰é€å­—ç¨¿
    python auto_watcher.py --once
    
    # æŒçºŒç›£æ§
    python auto_watcher.py
"""

import sys
import time
import json
import argparse
import re
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))
from podcast_pipeline import PodcastPipeline


# Whisper æª”åå‰ç¶´å°æ‡‰åˆ°ç¯€ç›®åç¨±
PREFIX_TO_PROGRAM = {
    "CFG": "è²¡å ±ç‹—",
    "MDJ": "Money DJ",
    "GY": "è‚¡ç™Œ",
    "MM": "Må¹³æ–¹",
}

# ç„¡å‰ç¶´æ™‚ï¼Œæ ¹æ“š EP ç·¨è™Ÿç¯„åœåˆ¤æ–·
def guess_program_from_ep(ep_num: int) -> str:
    """æ ¹æ“š EP ç·¨è™ŸçŒœæ¸¬ç¯€ç›®ï¼ˆç•¶æ²’æœ‰å‰ç¶´æ™‚ï¼‰"""
    if 290 <= ep_num <= 310:  # Må¹³æ–¹
        return "Må¹³æ–¹"
    elif 620 <= ep_num <= 640:  # è‚¡ç™Œ
        return "è‚¡ç™Œ"
    elif 580 <= ep_num <= 600:  # è²¡å ±ç‹—
        return "è²¡å ±ç‹—"
    elif 460 <= ep_num <= 470:  # Money DJ
        return "Money DJ"
    return ""


def parse_whisper_filename(filename: str) -> dict:
    """è§£æ Whisper é€å­—ç¨¿æª”å"""
    stem = filename.replace("_tw.txt", "").replace("_tw", "")
    
    # å˜—è©¦åŒ¹é…æœ‰å‰ç¶´çš„æ ¼å¼: CFG_EP585, MDJ_EP460, GY_EP627
    match = re.match(r"^([A-Z]+)_EP(\d+)$", stem)
    if match:
        prefix = match.group(1)
        ep_num = int(match.group(2))
        program = PREFIX_TO_PROGRAM.get(prefix, "")
        return {
            "stem": stem,
            "prefix": prefix,
            "ep_num": ep_num,
            "program": program,
            "canonical_name": f"{program}EP{ep_num}" if program else stem
        }
    
    # å˜—è©¦åŒ¹é…ç„¡å‰ç¶´çš„æ ¼å¼: EP296
    match = re.match(r"^EP(\d+)$", stem)
    if match:
        ep_num = int(match.group(1))
        program = guess_program_from_ep(ep_num)
        return {
            "stem": stem,
            "prefix": "",
            "ep_num": ep_num,
            "program": program,
            "canonical_name": f"{program}EP{ep_num}" if program else stem
        }
    
    # å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ç”¨åŸå
    return {
        "stem": stem,
        "prefix": "",
        "ep_num": 0,
        "program": "",
        "canonical_name": stem
    }


def update_sidebar(site_dir: Path, summaries_dir: Path):
    """æ›´æ–° sidebar.jsonï¼ˆåªä¿ç•™æ‘˜è¦ï¼‰"""
    sidebar_path = site_dir / ".vitepress" / "sidebar.json"
    
    program_names = ["Money DJ", "Må¹³æ–¹", "è‚¡ç™Œ", "è²¡å ±ç‹—"]
    
    summaries = {p: [] for p in program_names}
    
    # æƒææ‘˜è¦
    for f in sorted(summaries_dir.glob("*_summary.md"), reverse=True):
        name = f.stem.replace("_summary", "")
        for prog in program_names:
            if name.startswith(prog.replace(" ", "")):
                ep_match = re.search(r"EP(\d+)", name)
                if ep_match:
                    summaries[prog].append({
                        "text": f"EP{ep_match.group(1)}",
                        "link": f"/summaries/{f.name}"
                    })
                break
    
    sidebar = {
        "/summaries/": [{
            "text": "ç¯€ç›®åˆ—è¡¨",
            "items": [{"text": p, "collapsed": True, "items": summaries[p]} for p in program_names]
        }]
    }
    
    sidebar_path.write_text(json.dumps(sidebar, ensure_ascii=False, indent=2), encoding='utf-8')


def main():
    parser = argparse.ArgumentParser(description='Podcast è‡ªå‹•ç›£æ§å™¨ v4.0')
    parser.add_argument('--interval', type=int, default=60, help='ç›£æ§é–“éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--template', default='stock_analysis', help='æ‘˜è¦æ¨¡æ¿')
    parser.add_argument('--once', action='store_true', help='åªåŸ·è¡Œä¸€æ¬¡')
    args = parser.parse_args()
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸ”„ Podcast è‡ªå‹•ç›£æ§å™¨ v4.0                        â•‘
â•‘           (ç´”æ‘˜è¦ç‰ˆ - ä¸å†ç”Ÿæˆæ½¤ç¨¿é€å­—ç¨¿)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    pipeline = PodcastPipeline()
    
    if not pipeline.whisper.is_connected():
        print("âŒ ç„¡æ³•é€£æ¥ Whisper è³‡æ–™å¤¾ï¼")
        return
    
    summaries_dir = pipeline.summaries_dir
    site_summaries_dir = pipeline.site_summaries_dir
    
    print(f"ğŸ“‚ ç›£æ§ç›®éŒ„ï¼š{pipeline.whisper.output_dir}")
    print(f"{'â”€'*50}")
    
    # æ”¶é›†ç¾æœ‰çš„æ‘˜è¦
    existing_summaries = set()
    
    for f in summaries_dir.glob("*_summary.md"):
        name = f.stem.replace("_summary", "")
        existing_summaries.add(name)
    
    print(f"ğŸ“ ç¾æœ‰æ‘˜è¦ï¼š{len(existing_summaries)} å€‹")
    print(f"{'â”€'*50}\n")
    
    while True:
        now = datetime.now().strftime('%H:%M:%S')
        
        # æƒæ Whisper output
        whisper_files = list(pipeline.whisper.output_dir.glob('*_tw.txt'))
        
        need_summary = []
        
        for wf in whisper_files:
            info = parse_whisper_filename(wf.name)
            canonical = info["canonical_name"]
            
            # æª¢æŸ¥æ˜¯å¦ç¼ºæ‘˜è¦
            if canonical not in existing_summaries:
                need_summary.append((wf, info))
        
        if need_summary:
            print(f"\n[{now}] ğŸ“Š ç‹€æ…‹ï¼šéœ€è£œæ‘˜è¦ {len(need_summary)} å€‹")
        
        # è™•ç†éœ€è¦è£œçš„é …ç›®
        processed_count = 0
        
        for wf, info in need_summary:
            canonical = info["canonical_name"]
            
            print(f"\n[{now}] ğŸ“„ è™•ç†ï¼š{wf.name} â†’ {canonical}")
            
            # è®€å–é€å­—ç¨¿
            transcript = wf.read_text(encoding='utf-8')
            print(f"   ğŸ“„ é€å­—ç¨¿é•·åº¦ï¼š{len(transcript)} å­—")
            print(f"   ğŸ¤– ç”Ÿæˆæ‘˜è¦ä¸­...")
            
            result = pipeline.summarizer.process(
                transcript=transcript,
                episode_title=canonical,
                template_name=args.template
            )
            
            if result.success:
                # å„²å­˜æ‘˜è¦
                summary_path = summaries_dir / f"{canonical}_summary.md"
                summary_path.write_text(result.summary, encoding='utf-8')
                existing_summaries.add(canonical)
                
                # å„²å­˜æ‘˜è¦åˆ° siteï¼ˆå« frontmatterï¼‰
                site_summary = pipeline._add_frontmatter_to_summary(
                    result.summary, canonical, info["program"], "", canonical
                )
                site_summary_path = site_summaries_dir / f"{canonical}_summary.md"
                site_summary_path.write_text(site_summary, encoding='utf-8')
                
                print(f"   âœ… æ‘˜è¦å·²å„²å­˜ï¼š{summary_path.name}")
            else:
                print(f"   âŒ è™•ç†å¤±æ•—ï¼š{result.error}")
            
            processed_count += 1
        
        if processed_count > 0:
            # æ›´æ–° sidebar
            update_sidebar(pipeline.site_dir, site_summaries_dir)
            print(f"\n   ğŸ“‹ sidebar.json å·²æ›´æ–°")
        
        if not need_summary:
            print(f"[{now}] âœ… å…¨éƒ¨å®Œæˆï¼æ²’æœ‰ç¼ºæ‘˜è¦çš„é …ç›®", end='\r')
        
        if args.once:
            print(f"\n\nâœ… å–®æ¬¡åŸ·è¡Œå®Œæˆï¼è™•ç†äº† {processed_count} å€‹é …ç›®")
            break
        
        time.sleep(args.interval)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç›£æ§å·²åœæ­¢")
