#!/usr/bin/env python3
"""
ğŸ”„ Podcast è‡ªå‹•ç›£æ§å™¨ v3.0

åŠŸèƒ½ï¼š
- æƒæ Whisper output è³‡æ–™å¤¾
- æ™ºæ…§åˆ¤æ–·ï¼šç¼ºé€å­—ç¨¿è£œé€å­—ç¨¿ã€ç¼ºæ‘˜è¦è£œæ‘˜è¦
- è‡ªå‹•æ›´æ–° sidebar.json

ä½¿ç”¨æ–¹æ³•ï¼š
    # è™•ç†æ‰€æœ‰é€å­—ç¨¿ï¼ˆç¼ºå•¥è£œå•¥ï¼‰
    python auto_watcher.py --once
    
    # æŒçºŒç›£æ§
    python auto_watcher.py
"""

import sys
import time
import json
import argparse
import re
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))
from podcast_pipeline import PodcastPipeline


# Whisper æª”åå‰ç¶´å°æ‡‰åˆ°ç¯€ç›®åç¨±
PREFIX_TO_PROGRAM = {
    "CFG": "è²¡å ±ç‹—",
    "MDJ": "Money DJ",
    "GY": "è‚¡ç™Œ",
    "MM": "Må¹³æ–¹",
}

# ç„¡å‰ç¶´æ™‚ï¼Œæ ¹æ“š EP ç·¨è™Ÿç¯„åœåˆ¤æ–·
def guess_program_from_ep(ep_num: int) -> str:
    """æ ¹æ“š EP ç·¨è™ŸçŒœæ¸¬ç¯€ç›®ï¼ˆç•¶æ²’æœ‰å‰ç¶´æ™‚ï¼‰"""
    if 290 <= ep_num <= 310:  # Må¹³æ–¹
        return "Må¹³æ–¹"
    elif 620 <= ep_num <= 640:  # è‚¡ç™Œ
        return "è‚¡ç™Œ"
    elif 580 <= ep_num <= 600:  # è²¡å ±ç‹—
        return "è²¡å ±ç‹—"
    elif 460 <= ep_num <= 470:  # Money DJ
        return "Money DJ"
    return ""


def parse_whisper_filename(filename: str) -> dict:
    """è§£æ Whisper é€å­—ç¨¿æª”å"""
    stem = filename.replace("_tw.txt", "").replace("_tw", "")
    
    # å˜—è©¦åŒ¹é…æœ‰å‰ç¶´çš„æ ¼å¼: CFG_EP585, MDJ_EP460, GY_EP627
    match = re.match(r"^([A-Z]+)_EP(\d+)$", stem)
    if match:
        prefix = match.group(1)
        ep_num = int(match.group(2))
        program = PREFIX_TO_PROGRAM.get(prefix, "")
        return {
            "stem": stem,
            "prefix": prefix,
            "ep_num": ep_num,
            "program": program,
            "canonical_name": f"{program}EP{ep_num}" if program else stem
        }
    
    # å˜—è©¦åŒ¹é…ç„¡å‰ç¶´çš„æ ¼å¼: EP296
    match = re.match(r"^EP(\d+)$", stem)
    if match:
        ep_num = int(match.group(1))
        program = guess_program_from_ep(ep_num)
        return {
            "stem": stem,
            "prefix": "",
            "ep_num": ep_num,
            "program": program,
            "canonical_name": f"{program}EP{ep_num}" if program else stem
        }
    
    # å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ç”¨åŸå
    return {
        "stem": stem,
        "prefix": "",
        "ep_num": 0,
        "program": "",
        "canonical_name": stem
    }


def update_sidebar(site_dir: Path, summaries_dir: Path, transcripts_dir: Path):
    """æ›´æ–° sidebar.json"""
    sidebar_path = site_dir / ".vitepress" / "sidebar.json"
    
    program_names = ["Money DJ", "Må¹³æ–¹", "è‚¡ç™Œ", "è²¡å ±ç‹—"]
    
    summaries = {p: [] for p in program_names}
    transcripts = {p: [] for p in program_names}
    
    # æƒææ‘˜è¦
    for f in sorted(summaries_dir.glob("*_summary.md"), reverse=True):
        name = f.stem.replace("_summary", "")
        for prog in program_names:
            if name.startswith(prog.replace(" ", "")):
                ep_match = re.search(r"EP(\d+)", name)
                if ep_match:
                    summaries[prog].append({
                        "text": f"EP{ep_match.group(1)}",
                        "link": f"/summaries/{f.name}"
                    })
                break
    
    # æƒæé€å­—ç¨¿
    for f in sorted(transcripts_dir.glob("*_transcript.md"), reverse=True):
        name = f.stem.replace("_transcript", "")
        for prog in program_names:
            if name.startswith(prog.replace(" ", "")):
                ep_match = re.search(r"EP(\d+)", name)
                if ep_match:
                    transcripts[prog].append({
                        "text": f"EP{ep_match.group(1)}",
                        "link": f"/transcripts/{f.name}"
                    })
                break
    
    sidebar = {
        "/summaries/": [{
            "text": "ç¯€ç›®åˆ—è¡¨",
            "items": [{"text": p, "collapsed": True, "items": summaries[p]} for p in program_names]
        }],
        "/transcripts/": [{
            "text": "é€å­—ç¨¿åˆ—è¡¨",
            "items": [{"text": p, "collapsed": True, "items": transcripts[p]} for p in program_names]
        }]
    }
    
    sidebar_path.write_text(json.dumps(sidebar, ensure_ascii=False, indent=2), encoding='utf-8')


def main():
    parser = argparse.ArgumentParser(description='Podcast è‡ªå‹•ç›£æ§å™¨ v3.0')
    parser.add_argument('--interval', type=int, default=60, help='ç›£æ§é–“éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--template', default='stock_analysis', help='æ‘˜è¦æ¨¡æ¿')
    parser.add_argument('--once', action='store_true', help='åªåŸ·è¡Œä¸€æ¬¡')
    args = parser.parse_args()
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸ”„ Podcast è‡ªå‹•ç›£æ§å™¨ v3.0                        â•‘
â•‘           (ç¼ºå•¥è£œå•¥ æ™ºæ…§ç‰ˆ)                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    pipeline = PodcastPipeline()
    
    if not pipeline.whisper.is_connected():
        print("âŒ ç„¡æ³•é€£æ¥ Whisper è³‡æ–™å¤¾ï¼")
        return
    
    summaries_dir = pipeline.summaries_dir
    site_summaries_dir = pipeline.site_summaries_dir
    site_transcripts_dir = pipeline.site_transcripts_dir
    
    print(f"ğŸ“‚ ç›£æ§ç›®éŒ„ï¼š{pipeline.whisper.output_dir}")
    print(f"{'â”€'*50}")
    
    # æ”¶é›†ç¾æœ‰çš„æ‘˜è¦å’Œé€å­—ç¨¿ï¼ˆç”¨ canonical_nameï¼‰
    existing_summaries = set()
    existing_transcripts = set()
    
    for f in summaries_dir.glob("*_summary.md"):
        name = f.stem.replace("_summary", "")
        existing_summaries.add(name)
    
    for f in site_transcripts_dir.glob("*_transcript.md"):
        name = f.stem.replace("_transcript", "")
        existing_transcripts.add(name)
    
    print(f"ğŸ“ ç¾æœ‰æ‘˜è¦ï¼š{len(existing_summaries)} å€‹")
    print(f"ğŸ“„ ç¾æœ‰é€å­—ç¨¿ï¼š{len(existing_transcripts)} å€‹")
    print(f"{'â”€'*50}\n")
    
    while True:
        now = datetime.now().strftime('%H:%M:%S')
        
        # æƒæ Whisper output
        whisper_files = list(pipeline.whisper.output_dir.glob('*_tw.txt'))
        
        need_transcript = []
        need_summary = []
        
        for wf in whisper_files:
            info = parse_whisper_filename(wf.name)
            canonical = info["canonical_name"]
            
            # æª¢æŸ¥ç¼ºä»€éº¼
            has_summary = canonical in existing_summaries
            has_transcript = canonical in existing_transcripts
            
            if not has_transcript:
                need_transcript.append((wf, info))
            if not has_summary:
                need_summary.append((wf, info))
        
        # çµ±è¨ˆ
        total_tasks = len([x for x in need_transcript if x not in [(nf, ni) for nf, ni in need_summary]]) + len(need_summary)
        
        if need_transcript or need_summary:
            print(f"\n[{now}] ğŸ“Š ç‹€æ…‹ï¼š")
            print(f"   éœ€è£œé€å­—ç¨¿ï¼š{len(need_transcript)} å€‹")
            print(f"   éœ€è£œæ‘˜è¦ï¼š{len(need_summary)} å€‹")
        
        # è™•ç†éœ€è¦è£œçš„é …ç›®
        processed_count = 0
        
        for wf, info in need_transcript:
            canonical = info["canonical_name"]
            has_summary = canonical in existing_summaries
            
            print(f"\n[{now}] ğŸ“„ è™•ç†ï¼š{wf.name} â†’ {canonical}")
            
            # è®€å–é€å­—ç¨¿
            transcript = wf.read_text(encoding='utf-8')
            print(f"   ğŸ“„ é€å­—ç¨¿é•·åº¦ï¼š{len(transcript)} å­—")
            
            if has_summary:
                # åªéœ€è¦è£œé€å­—ç¨¿ï¼ˆåªè·‘æ½¤ç¨¿ï¼‰
                print(f"   âœ… å·²æœ‰æ‘˜è¦ï¼Œåªè£œæ½¤ç¨¿é€å­—ç¨¿...")
                
                polish_result = pipeline.summarizer.polish_transcript(transcript, args.template)
                
                if polish_result.success:
                    polished = polish_result.content
                    transcript_md = pipeline.summarizer.format_transcript_for_display(
                        polished,
                        canonical,
                        info["program"],
                        ""
                    )
                    transcript_path = site_transcripts_dir / f"{canonical}_transcript.md"
                    transcript_path.write_text(transcript_md, encoding='utf-8')
                    existing_transcripts.add(canonical)
                    print(f"   âœ… é€å­—ç¨¿å·²å„²å­˜ï¼š{transcript_path.name}")
                else:
                    print(f"   âŒ æ½¤ç¨¿å¤±æ•—ï¼š{polish_result.error}")
            else:
                # éœ€è¦è·‘å®Œæ•´æµç¨‹ï¼ˆæ½¤ç¨¿ + æ‘˜è¦ï¼‰
                print(f"   ğŸ¤– å®Œæ•´è™•ç†ï¼ˆæ½¤ç¨¿ + æ‘˜è¦ï¼‰...")
                
                result = pipeline.summarizer.process(
                    transcript=transcript,
                    episode_title=canonical,
                    template_name=args.template
                )
                
                if result.success:
                    # å„²å­˜æ‘˜è¦
                    summary_path = summaries_dir / f"{canonical}_summary.md"
                    summary_path.write_text(result.summary, encoding='utf-8')
                    existing_summaries.add(canonical)
                    
                    # å„²å­˜æ‘˜è¦åˆ° siteï¼ˆå« frontmatterï¼‰
                    site_summary = pipeline._add_frontmatter_to_summary(
                        result.summary, canonical, info["program"], "", canonical
                    )
                    site_summary_path = site_summaries_dir / f"{canonical}_summary.md"
                    site_summary_path.write_text(site_summary, encoding='utf-8')
                    
                    # å„²å­˜é€å­—ç¨¿
                    if result.polished_transcript:
                        transcript_md = pipeline.summarizer.format_transcript_for_display(
                            result.polished_transcript,
                            canonical,
                            info["program"],
                            ""
                        )
                        transcript_path = site_transcripts_dir / f"{canonical}_transcript.md"
                        transcript_path.write_text(transcript_md, encoding='utf-8')
                        existing_transcripts.add(canonical)
                        print(f"   âœ… é€å­—ç¨¿å·²å„²å­˜ï¼š{transcript_path.name}")
                    
                    print(f"   âœ… æ‘˜è¦å·²å„²å­˜ï¼š{summary_path.name}")
                else:
                    print(f"   âŒ è™•ç†å¤±æ•—ï¼š{result.error}")
            
            processed_count += 1
        
        if processed_count > 0:
            # æ›´æ–° sidebar
            update_sidebar(pipeline.site_dir, site_summaries_dir, site_transcripts_dir)
            print(f"\n   ğŸ“‹ sidebar.json å·²æ›´æ–°")
        
        if not need_transcript and not need_summary:
            print(f"[{now}] âœ… å…¨éƒ¨å®Œæˆï¼æ²’æœ‰ç¼ºæ¼çš„é …ç›®", end='\r')
        
        if args.once:
            print(f"\n\nâœ… å–®æ¬¡åŸ·è¡Œå®Œæˆï¼è™•ç†äº† {processed_count} å€‹é …ç›®")
            break
        
        time.sleep(args.interval)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç›£æ§å·²åœæ­¢")
