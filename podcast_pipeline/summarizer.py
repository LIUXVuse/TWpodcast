"""
Summarizer - æ‘˜è¦ç”Ÿæˆå™¨

è² è²¬ï¼š
1. è¼‰å…¥å’Œç®¡ç†æ‘˜è¦æ¨¡æ¿
2. ä½¿ç”¨ Ollama é€²è¡Œé€å­—ç¨¿æ½¤ç¨¿
3. ç”Ÿæˆçµæ§‹åŒ–æ‘˜è¦
"""

import yaml
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

from .ollama_client import OllamaClient, LLMResponse


@dataclass
class SummaryResult:
    """æ‘˜è¦çµæœ"""
    success: bool
    polished_transcript: Optional[str] = None
    summary: Optional[str] = None
    template_used: Optional[str] = None
    error: Optional[str] = None


class Summarizer:
    """æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, ollama_client: OllamaClient, templates_path: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
        
        Args:
            ollama_client: Ollama å®¢æˆ¶ç«¯å¯¦ä¾‹
            templates_path: æ¨¡æ¿è¨­å®šæª”è·¯å¾‘
        """
        self.ollama = ollama_client
        self.templates_path = templates_path or Path(__file__).parent.parent / "config" / "templates.yaml"
        self.templates = self._load_templates()
    
    def _load_templates(self) -> dict:
        """è¼‰å…¥æ¨¡æ¿è¨­å®š"""
        if self.templates_path.exists():
            with open(self.templates_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get('templates', {})
        return self._get_default_templates()
    
    def _get_default_templates(self) -> dict:
        """å–å¾—é è¨­æ¨¡æ¿"""
        return {
            'stock_analysis': {
                'name': 'è‚¡ç¥¨è²¡ç¶“åˆ†æ',
                'description': 'å°ˆæ³¨æ–¼æå–è‚¡ç¥¨ã€å…¬å¸ã€æ•¸å­—ã€å±•æœ›ç­‰è²¡ç¶“è³‡è¨Š',
                'polish_prompt': '''ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç·¨è¼¯ã€‚è«‹å¹«æˆ‘æ½¤é£¾ä»¥ä¸‹ Podcast é€å­—ç¨¿ï¼š

1. ä¿®æ­£æ˜é¡¯çš„èªéŸ³è¾¨è­˜éŒ¯èª¤
2. è£œä¸Šé©ç•¶çš„æ¨™é»ç¬¦è™Ÿ
3. ä¿ç•™åŸæ„ï¼Œä¸è¦å¤§å¹…ä¿®æ”¹å…§å®¹
4. å°ˆæœ‰åè©ï¼ˆå…¬å¸åã€è‚¡ç¥¨ä»£è™Ÿï¼‰è¦æ­£ç¢º

é€å­—ç¨¿å…§å®¹ï¼š
{transcript}

è«‹ç›´æ¥è¼¸å‡ºæ½¤é£¾å¾Œçš„æ–‡å­—ï¼Œä¸éœ€è¦é¡å¤–èªªæ˜ã€‚''',
                'summary_prompt': '''ä½ æ˜¯ä¸€ä½è²¡ç¶“ Podcast æ‘˜è¦å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹é€å­—ç¨¿ï¼Œç”¢ç”Ÿçµæ§‹åŒ–çš„æ‘˜è¦ã€‚

ç‰¹åˆ¥æ³¨æ„æå–ï¼š
- ğŸ“ˆ æåˆ°çš„**å…¬å¸åç¨±**å’Œ**è‚¡ç¥¨ä»£è™Ÿ**
- ğŸ“Š æåˆ°çš„**å…·é«”æ•¸å­—**ï¼ˆè‚¡åƒ¹ã€æ¼²è·Œå¹…ã€ç‡Ÿæ”¶ã€EPS ç­‰ï¼‰
- ğŸ”® å°å¸‚å ´æˆ–å…¬å¸çš„**å±•æœ›å’Œé æ¸¬**
- ğŸ“° é‡è¦çš„**æ–°èäº‹ä»¶**æˆ–**ç”¢æ¥­å‹•æ…‹**

é€å­—ç¨¿ï¼š
{transcript}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆMarkdownï¼‰ï¼š

# {episode_title}

## ğŸ“Œ ä¸€å¥è©±æ‘˜è¦
ï¼ˆç”¨ä¸€å¥è©±æ¦‚æ‹¬é€™é›†çš„æ ¸å¿ƒå…§å®¹ï¼‰

## ğŸ“ˆ æåŠçš„å…¬å¸èˆ‡è‚¡ç¥¨
| å…¬å¸/è‚¡ç¥¨ | ä»£è™Ÿ | ç›¸é—œæ•¸æ“š | å±•æœ›/è©•è«– |
|---------|------|---------|----------|
| ... | ... | ... | ... |

## ğŸ¯ é‡é»æ•´ç†
1. ...
2. ...
3. ...

## ğŸ“ è©³ç´°å…§å®¹

### è©±é¡Œä¸€ï¼š...
- ...

### è©±é¡ŒäºŒï¼š...
- ...

## ğŸ’¡ é‡‘å¥
> "..."

## ğŸ”® å±•æœ›èˆ‡é æ¸¬
- ...'''
            },
            'default': {
                'name': 'é€šç”¨æ‘˜è¦',
                'description': 'é©ç”¨æ–¼å„é¡å‹ Podcast',
                'polish_prompt': '''ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç·¨è¼¯ã€‚è«‹å¹«æˆ‘æ½¤é£¾ä»¥ä¸‹ Podcast é€å­—ç¨¿ï¼š

1. ä¿®æ­£æ˜é¡¯çš„èªéŸ³è¾¨è­˜éŒ¯èª¤
2. è£œä¸Šé©ç•¶çš„æ¨™é»ç¬¦è™Ÿ
3. ä¿ç•™åŸæ„ï¼Œä¸è¦å¤§å¹…ä¿®æ”¹å…§å®¹

é€å­—ç¨¿å…§å®¹ï¼š
{transcript}

è«‹ç›´æ¥è¼¸å‡ºæ½¤é£¾å¾Œçš„æ–‡å­—ï¼Œä¸éœ€è¦é¡å¤–èªªæ˜ã€‚''',
                'summary_prompt': '''ä½ æ˜¯ä¸€ä½ Podcast æ‘˜è¦å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹é€å­—ç¨¿ï¼Œç”¢ç”Ÿçµæ§‹åŒ–çš„æ‘˜è¦ã€‚

é€å­—ç¨¿ï¼š
{transcript}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆMarkdownï¼‰ï¼š

# {episode_title}

## ğŸ“Œ ä¸€å¥è©±æ‘˜è¦
ï¼ˆç”¨ä¸€å¥è©±æ¦‚æ‹¬é€™é›†çš„æ ¸å¿ƒå…§å®¹ï¼‰

## ğŸ¯ é‡é»æ•´ç†
1. ...
2. ...
3. ...

## ğŸ“ è©³ç´°å…§å®¹

### è©±é¡Œä¸€ï¼š...
- ...

### è©±é¡ŒäºŒï¼š...
- ...

## ğŸ’¡ é‡‘å¥
> "..."'''
            }
        }
    
    def reload_templates(self):
        """é‡æ–°è¼‰å…¥æ¨¡æ¿"""
        self.templates = self._load_templates()
    
    def get_template_names(self) -> list[str]:
        """å–å¾—æ‰€æœ‰æ¨¡æ¿åç¨±"""
        return list(self.templates.keys())
    
    def get_template_info(self, template_name: str) -> Optional[dict]:
        """å–å¾—æ¨¡æ¿è³‡è¨Š"""
        template = self.templates.get(template_name)
        if template:
            return {
                'name': template.get('name', template_name),
                'description': template.get('description', '')
            }
        return None
    
    def polish_transcript(
        self, 
        transcript: str,
        template_name: str = 'default'
    ) -> LLMResponse:
        """
        æ½¤é£¾é€å­—ç¨¿
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            
        Returns:
            LLMResponse ç‰©ä»¶
        """
        # å¦‚æœé€å­—ç¨¿å¤ªé•·ï¼Œä½¿ç”¨åˆ†æ®µè™•ç†
        if len(transcript) > 8000:
            return self.polish_transcript_chunked(transcript, template_name)
        
        template = self.templates.get(template_name, self.templates.get('default'))
        
        if not template:
            return LLMResponse(success=False, error=f"æ‰¾ä¸åˆ°æ¨¡æ¿ï¼š{template_name}")
        
        prompt = template['polish_prompt'].format(transcript=transcript)
        
        print(f"âœ¨ é–‹å§‹æ½¤ç¨¿ï¼ˆä½¿ç”¨æ¨¡æ¿ï¼š{template.get('name', template_name)}ï¼‰...")
        return self.ollama.generate(prompt, timeout=600)
    
    def polish_transcript_chunked(
        self,
        transcript: str,
        template_name: str = 'default',
        chunk_size: int = 6000,
        overlap: int = 500
    ) -> LLMResponse:
        """
        åˆ†æ®µæ½¤é£¾é•·é€å­—ç¨¿
        
        å°‡é•·é€å­—ç¨¿åˆ‡æˆå¤šæ®µï¼Œåˆ†åˆ¥æ½¤ç¨¿å¾Œåˆä½µã€‚
        ä½¿ç”¨é‡ç–Šçª—å£ç¢ºä¿é‚Šç•Œè³‡è¨Šä¸æœƒéºå¤±ã€‚
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            chunk_size: æ¯æ®µå¤§å°ï¼ˆå­—æ•¸ï¼‰
            overlap: é‡ç–Šå€åŸŸå¤§å°ï¼ˆå­—æ•¸ï¼‰
            
        Returns:
            LLMResponse ç‰©ä»¶
        """
        # åˆ†æ®µ
        chunks = []
        start = 0
        while start < len(transcript):
            end = min(start + chunk_size, len(transcript))
            chunks.append(transcript[start:end])
            start = end - overlap  # é‡ç–Š
            if start >= len(transcript) - overlap:
                break
        
        print(f"âœ¨ é–‹å§‹åˆ†æ®µæ½¤ç¨¿ï¼ˆå…± {len(chunks)} æ®µï¼Œä½¿ç”¨æ¨¡æ¿ï¼š{template_name}ï¼‰...")
        
        # å»ºç«‹åˆ†æ®µå°ˆç”¨çš„ promptï¼ˆæ›´ç°¡å–®ï¼Œåªåšæ½¤é£¾ä¸åšç« ç¯€ï¼‰
        chunk_prompt_template = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç·¨è¼¯ã€‚è«‹æ½¤é£¾ä»¥ä¸‹ Podcast é€å­—ç¨¿ç‰‡æ®µã€‚

âš ï¸ é‡è¦è¦å‰‡ï¼š
1. ä½ å¿…é ˆè¼¸å‡ºã€å®Œæ•´å…§å®¹ã€‘ï¼Œä¸å¯ä»¥åˆªæ¸›æˆ–çœç•¥ä»»ä½•å…§å®¹
2. é€™æ˜¯é€å­—ç¨¿çš„ä¸€å€‹ç‰‡æ®µï¼Œè«‹ä¿æŒåŸæ¨£ï¼Œåªåšä»¥ä¸‹ä¿®æ”¹ï¼š
   - ä¿®æ­£èªéŸ³è¾¨è­˜éŒ¯èª¤
   - è£œä¸Šæ¨™é»ç¬¦è™Ÿ
   - ä¿®æ­£å°ˆæœ‰åè©
3. ä¸è¦åŠ ä»»ä½•æ¨™é¡Œã€ç« ç¯€ã€æˆ–æ ¼å¼
4. ç›´æ¥è¼¸å‡ºæ½¤é£¾å¾Œçš„æ–‡å­—

é€å­—ç¨¿ç‰‡æ®µï¼š
{transcript}

è«‹è¼¸å‡ºæ½¤é£¾å¾Œçš„å®Œæ•´æ–‡å­—ï¼š"""

        polished_chunks = []
        
        for i, chunk in enumerate(chunks):
            print(f"   ğŸ“„ è™•ç†ç¬¬ {i+1}/{len(chunks)} æ®µï¼ˆ{len(chunk)} å­—ï¼‰...")
            
            prompt = chunk_prompt_template.format(transcript=chunk)
            result = self.ollama.generate(prompt, timeout=600)
            
            if result.success:
                polished_chunks.append(result.content)
            else:
                print(f"   âš ï¸ ç¬¬ {i+1} æ®µè™•ç†å¤±æ•—ï¼š{result.error}")
                polished_chunks.append(chunk)  # å¤±æ•—æ™‚ä½¿ç”¨åŸæ–‡
        
        # åˆä½µï¼ˆå»é™¤é‡ç–Šéƒ¨åˆ†çš„é‡è¤‡ï¼‰
        merged = polished_chunks[0] if polished_chunks else ""
        for i in range(1, len(polished_chunks)):
            # ç°¡å–®åˆä½µï¼Œå› ç‚º LLM è¼¸å‡ºçš„é‡ç–Šéƒ¨åˆ†å¯èƒ½ä¸å®Œå…¨ç›¸åŒ
            # ç›´æ¥æ‹¼æ¥ï¼Œè®“å…§å®¹å®Œæ•´
            merged += "\n\n" + polished_chunks[i]
        
        # æ•´ç†æ ¼å¼ï¼šåŠ ä¸Šç« ç¯€
        print(f"   ğŸ“ æ•´ç†æ ¼å¼ä¸¦åŠ å…¥ç« ç¯€...")
        format_prompt = f"""è«‹å°‡ä»¥ä¸‹å·²æ½¤é£¾çš„é€å­—ç¨¿æ•´ç†æˆ Markdown æ ¼å¼ï¼ŒåŠ ä¸Šç« ç¯€æ¨™é¡Œã€‚

è¦å‰‡ï¼š
1. ä¿ç•™æ‰€æœ‰å…§å®¹ï¼Œä¸å¯åˆªæ¸›
2. ç”¨ `## ğŸ¯ æ¨™é¡Œ` æ ¼å¼åˆ†éš”ä¸åŒè©±é¡Œ
3. å¯ç”¨çš„ç« ç¯€é¡å‹ï¼šé–‹å ´ã€å»£å‘Šæ¥­é…ã€ä¸»é¡Œè¨è«–ã€è½çœ¾å•ç­”ã€çµå°¾
4. æ®µè½ä¹‹é–“ç©ºä¸€è¡Œ

å·²æ½¤é£¾çš„é€å­—ç¨¿ï¼š
{merged}

è«‹è¼¸å‡ºå®Œæ•´çš„ Markdown æ ¼å¼é€å­—ç¨¿ï¼š"""

        final_result = self.ollama.generate(format_prompt, timeout=600)
        
        if final_result.success:
            print(f"   âœ… åˆ†æ®µæ½¤ç¨¿å®Œæˆï¼ˆåŸå§‹ {len(transcript)} å­— â†’ è¼¸å‡º {len(final_result.content)} å­—ï¼‰")
            return final_result
        else:
            # å¦‚æœæ ¼å¼åŒ–å¤±æ•—ï¼Œè¿”å›åˆä½µçµæœ
            return LLMResponse(
                success=True,
                content=merged,
                model=final_result.model
            )
    
    def generate_summary(
        self,
        transcript: str,
        episode_title: str,
        template_name: str = 'default'
    ) -> LLMResponse:
        """
        ç”Ÿæˆæ‘˜è¦
        
        Args:
            transcript: é€å­—ç¨¿ï¼ˆå»ºè­°å…ˆæ½¤ç¨¿ï¼‰
            episode_title: é›†æ•¸æ¨™é¡Œ
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            
        Returns:
            LLMResponse ç‰©ä»¶
        """
        template = self.templates.get(template_name, self.templates.get('default'))
        
        if not template:
            return LLMResponse(success=False, error=f"æ‰¾ä¸åˆ°æ¨¡æ¿ï¼š{template_name}")
        
        prompt = template['summary_prompt'].format(
            transcript=transcript,
            episode_title=episode_title
        )
        
        print(f"ğŸ“ é–‹å§‹ç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨æ¨¡æ¿ï¼š{template.get('name', template_name)}ï¼‰...")
        return self.ollama.generate(prompt, timeout=600)
    
    def process(
        self,
        transcript: str,
        episode_title: str,
        template_name: str = 'stock_analysis',
        skip_polish: bool = False
    ) -> SummaryResult:
        """
        å®Œæ•´è™•ç†æµç¨‹ï¼šæ½¤ç¨¿ + æ‘˜è¦
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            episode_title: é›†æ•¸æ¨™é¡Œ
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            skip_polish: æ˜¯å¦è·³éæ½¤ç¨¿æ­¥é©Ÿ
            
        Returns:
            SummaryResult ç‰©ä»¶
        """
        polished = transcript
        
        # æ­¥é©Ÿ 1ï¼šæ½¤ç¨¿ï¼ˆå¯é¸ï¼‰
        if not skip_polish:
            polish_result = self.polish_transcript(transcript, template_name)
            if polish_result.success:
                polished = polish_result.content
                print(f"âœ… æ½¤ç¨¿å®Œæˆï¼ˆä½¿ç”¨æ¨¡å‹ï¼š{polish_result.model}ï¼‰")
            else:
                print(f"âš ï¸ æ½¤ç¨¿å¤±æ•—ï¼š{polish_result.error}ï¼Œä½¿ç”¨åŸå§‹é€å­—ç¨¿ç¹¼çºŒ")
        
        # æ­¥é©Ÿ 2ï¼šç”Ÿæˆæ‘˜è¦
        summary_result = self.generate_summary(polished, episode_title, template_name)
        
        if summary_result.success:
            print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆä½¿ç”¨æ¨¡å‹ï¼š{summary_result.model}ï¼‰")
            return SummaryResult(
                success=True,
                polished_transcript=polished,
                summary=summary_result.content,
                template_used=template_name
            )
        else:
            return SummaryResult(
                success=False,
                polished_transcript=polished,
                error=f"æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{summary_result.error}"
            )
    
    def save_template(self, template_name: str, template_data: dict) -> bool:
        """
        å„²å­˜è‡ªè¨‚æ¨¡æ¿
        
        Args:
            template_name: æ¨¡æ¿åç¨±ï¼ˆè‹±æ–‡ï¼‰
            template_data: æ¨¡æ¿å…§å®¹ï¼ŒåŒ…å« name, description, polish_prompt, summary_prompt
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # è®€å–ç¾æœ‰è¨­å®š
            if self.templates_path.exists():
                with open(self.templates_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f) or {}
            else:
                data = {'templates': {}}
            
            # æ›´æ–°æ¨¡æ¿
            if 'templates' not in data:
                data['templates'] = {}
            
            data['templates'][template_name] = template_data
            
            # å¯«å…¥æª”æ¡ˆ
            with open(self.templates_path, 'w', encoding='utf-8') as f:
                yaml.dump(data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
            
            # é‡æ–°è¼‰å…¥
            self.reload_templates()
            
            print(f"âœ… æ¨¡æ¿ '{template_name}' å·²å„²å­˜")
            return True
            
        except Exception as e:
            print(f"âŒ å„²å­˜æ¨¡æ¿å¤±æ•—ï¼š{e}")
            return False


    def format_transcript_for_display(
        self,
        polished_transcript: str,
        episode_title: str,
        podcast_name: str = "",
        audio_url: str = ""
    ) -> str:
        """
        å°‡æ½¤ç¨¿å¾Œçš„é€å­—ç¨¿æ ¼å¼åŒ–ç‚º Markdown é¡¯ç¤ºæ ¼å¼
        
        Args:
            polished_transcript: æ½¤ç¨¿å¾Œçš„é€å­—ç¨¿
            episode_title: é›†æ•¸æ¨™é¡Œ
            podcast_name: Podcast åç¨±
            audio_url: éŸ³è¨Š URLï¼ˆç”¨æ–¼æ’­æ”¾å™¨ï¼‰
            
        Returns:
            æ ¼å¼åŒ–å¾Œçš„ Markdown å…§å®¹
        """
        import re
        
        # æ¸…ç† LLM è¼¸å‡ºçš„ä»£ç¢¼å¡Šæ¨™è¨˜
        content = polished_transcript.strip()
        
        # ç§»é™¤é–‹é ­çš„ ```markdown æˆ– ```
        content = re.sub(r'^```(?:markdown|md)?\s*\n?', '', content)
        # ç§»é™¤çµå°¾çš„ ```
        content = re.sub(r'\n?```\s*$', '', content)
        
        # å»ºç«‹ frontmatter
        frontmatter = f"""---
title: "{episode_title} - é€å­—ç¨¿"
podcast: "{podcast_name}"
audioUrl: "{audio_url}"
---

"""
        
        # æ¨™é¡Œå€å¡Š
        header = f"# ğŸ“ {episode_title}\n\n"
        if podcast_name:
            header += f"> ğŸ“» ç¯€ç›®ï¼š{podcast_name}\n\n"
        header += "---\n\n"
        
        # ç¢ºä¿å…§å®¹æœ‰æ­£ç¢ºçš„ Markdown æ ¼å¼
        # å¦‚æœå…§å®¹æ²’æœ‰ç« ç¯€æ¨™é¡Œï¼ŒåŠ ä¸Šä¸€å€‹
        if not content.startswith('#'):
            content = "## å®Œæ•´é€å­—ç¨¿\n\n" + content
        
        return frontmatter + header + content


# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    from .ollama_client import OllamaClient
    
    ollama_config = {
        'local': {
            'primary_url': 'http://localhost:11434',
            'model': 'gemma3:27b'
        },
        'cloud': {'enabled': False},
        'priority': ['local']
    }
    
    ollama = OllamaClient(ollama_config)
    summarizer = Summarizer(ollama)
    
    print("ğŸ“‹ å¯ç”¨æ¨¡æ¿ï¼š")
    for name in summarizer.get_template_names():
        info = summarizer.get_template_info(name)
        print(f"  - {name}: {info['name']} - {info['description']}")
