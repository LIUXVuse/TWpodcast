"""
Ollama Client - LLM æ•´åˆå®¢æˆ¶ç«¯

æ”¯æ´ï¼š
1. æœ¬åœ° Ollamaï¼ˆWindows 5090 æˆ– Macï¼‰
2. Ollama Cloudï¼ˆå…è²» DeepSeek V3.1ï¼‰
"""

import requests
import time
from typing import Optional, Generator
from dataclasses import dataclass


@dataclass
class LLMResponse:
    """LLM å›æ‡‰"""
    success: bool
    content: Optional[str] = None
    model: Optional[str] = None
    source: Optional[str] = None  # 'local' or 'cloud'
    error: Optional[str] = None
    tokens_used: Optional[int] = None


class OllamaClient:
    """Ollama LLM å®¢æˆ¶ç«¯"""
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ– Ollama Client
        
        Args:
            config: è¨­å®šå­—å…¸ï¼ŒåŒ…å« local å’Œ cloud è¨­å®š
        """
        self.local_config = config.get('local', {})
        self.cloud_config = config.get('cloud', {})
        self.priority = config.get('priority', ['local', 'cloud'])
        
        # æœ¬åœ°è¨­å®š
        self.local_primary_url = self.local_config.get('primary_url', 'http://localhost:11434')
        self.local_fallback_url = self.local_config.get('fallback_url', 'http://localhost:11434')
        
        # æ¨¡å‹å„ªå…ˆé †åºï¼ˆæ”¯æ´åˆ—è¡¨æˆ–å–®ä¸€æ¨¡å‹ï¼‰
        models = self.local_config.get('models', [])
        if isinstance(models, list) and models:
            self.local_models = models
        else:
            # å‘å¾Œç›¸å®¹ï¼šå–®ä¸€æ¨¡å‹
            self.local_models = [self.local_config.get('model', 'gemma3:27b')]
        
        # æ¨¡å‹å†·å»è¿½è¹¤ï¼ˆé™æµå¾Œ 2 å°æ™‚å…§ä¸å†å˜—è©¦ï¼‰
        self.model_cooldown = {}  # {model_name: cooldown_until_timestamp}
        self.cooldown_duration = 2 * 60 * 60  # 2 å°æ™‚
        
        # é›²ç«¯è¨­å®š
        self.cloud_enabled = self.cloud_config.get('enabled', False)
        self.cloud_url = self.cloud_config.get('url', 'https://api.ollama.com/v1')
        self.cloud_model = self.cloud_config.get('model', 'deepseek-v3.1:671b-cloud')
        self.cloud_api_key = self.cloud_config.get('api_key', '')
    
    def test_connection(self, url: str) -> bool:
        """æ¸¬è©¦ Ollama é€£æ¥"""
        try:
            resp = requests.get(f"{url}/api/tags", timeout=5)
            return resp.ok
        except:
            return False
    
    def get_available_models(self, url: str) -> list[str]:
        """å–å¾—å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            resp = requests.get(f"{url}/api/tags", timeout=10)
            if resp.ok:
                data = resp.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []
    
    def _generate_local(
        self, 
        prompt: str, 
        url: str,
        model: Optional[str] = None,
        timeout: int = 300
    ) -> LLMResponse:
        """æœ¬åœ° Ollama ç”Ÿæˆ"""
        model = model or self.local_model
        
        try:
            resp = requests.post(
                f"{url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=timeout
            )
            
            if resp.ok:
                data = resp.json()
                return LLMResponse(
                    success=True,
                    content=data.get('response', ''),
                    model=model,
                    source='local',
                    tokens_used=data.get('eval_count')
                )
            else:
                return LLMResponse(
                    success=False,
                    error=f"HTTP {resp.status_code}: {resp.text[:200]}"
                )
        except requests.Timeout:
            return LLMResponse(success=False, error="è«‹æ±‚è¶…æ™‚")
        except requests.RequestException as e:
            return LLMResponse(success=False, error=str(e))
    
    def _generate_cloud(
        self, 
        prompt: str,
        timeout: int = 300
    ) -> LLMResponse:
        """Ollama Cloud ç”Ÿæˆ"""
        if not self.cloud_enabled:
            return LLMResponse(success=False, error="é›²ç«¯æœå‹™æœªå•Ÿç”¨")
        
        headers = {}
        if self.cloud_api_key:
            headers['Authorization'] = f'Bearer {self.cloud_api_key}'
        
        try:
            # Ollama Cloud ä½¿ç”¨é¡ä¼¼ OpenAI çš„ API
            resp = requests.post(
                f"{self.cloud_url}/chat/completions",
                headers=headers,
                json={
                    "model": self.cloud_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                },
                timeout=timeout
            )
            
            if resp.ok:
                data = resp.json()
                content = data['choices'][0]['message']['content']
                return LLMResponse(
                    success=True,
                    content=content,
                    model=self.cloud_model,
                    source='cloud',
                    tokens_used=data.get('usage', {}).get('total_tokens')
                )
            else:
                return LLMResponse(
                    success=False,
                    error=f"HTTP {resp.status_code}: {resp.text[:200]}"
                )
        except requests.Timeout:
            return LLMResponse(success=False, error="é›²ç«¯è«‹æ±‚è¶…æ™‚")
        except requests.RequestException as e:
            return LLMResponse(success=False, error=str(e))
    
    def generate(
        self, 
        prompt: str,
        model: Optional[str] = None,
        timeout: int = 300,
        retry_count: int = 2
    ) -> LLMResponse:
        """
        ç”Ÿæˆæ–‡å­—ï¼ŒæŒ‰å„ªå…ˆé †åºå˜—è©¦ä¸åŒæœå‹™å’Œæ¨¡å‹
        
        Args:
            prompt: è¼¸å…¥æç¤ºè©
            model: æŒ‡å®šæ¨¡å‹ï¼ˆå¯é¸ï¼ŒæŒ‡å®šæ™‚ä¸æœƒå˜—è©¦å…¶ä»–æ¨¡å‹ï¼‰
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            retry_count: æ¯å€‹æœå‹™çš„é‡è©¦æ¬¡æ•¸
            
        Returns:
            LLMResponse ç‰©ä»¶
        """
        errors = []
        
        # æ±ºå®šè¦å˜—è©¦çš„æ¨¡å‹åˆ—è¡¨
        models_to_try = [model] if model else self.local_models
        
        # éæ¿¾æ‰ä»åœ¨å†·å»ä¸­çš„æ¨¡å‹
        current_time = time.time()
        available_models = []
        for m in models_to_try:
            cooldown_until = self.model_cooldown.get(m, 0)
            if current_time >= cooldown_until:
                available_models.append(m)
            else:
                remaining = int((cooldown_until - current_time) / 60)
                print(f"â³ {m} å†·å»ä¸­ï¼Œå‰©é¤˜ {remaining} åˆ†é˜ï¼Œè·³é")
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½åœ¨å†·å»ï¼Œæ¸…é™¤æœ€èˆŠçš„å†·å»
        if not available_models and models_to_try:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å†·å»ä¸­ï¼Œé‡ç½®å†·å»ç‹€æ…‹")
            self.model_cooldown.clear()
            available_models = models_to_try
        
        for source in self.priority:
            if source == 'local':
                # å˜—è©¦æœ¬åœ°æœå‹™
                for url in [self.local_primary_url, self.local_fallback_url]:
                    if not self.test_connection(url):
                        errors.append(f"[local:{url}] ç„¡æ³•é€£æ¥")
                        continue
                    
                    # å˜—è©¦æ¯å€‹æ¨¡å‹
                    for try_model in available_models:
                        for attempt in range(retry_count):
                            print(f"ğŸ¤– å˜—è©¦ {try_model} @ {url}... (ç¬¬ {attempt + 1} æ¬¡)")
                            result = self._generate_local(prompt, url, try_model, timeout)
                            
                            if result.success:
                                return result
                            
                            errors.append(f"[{try_model}@{url.split('/')[-1]}] {result.error}")
                            
                            # å¦‚æœæ˜¯ 429 é™æµéŒ¯èª¤ï¼Œè¨­å®šå†·å»ä¸¦æ›ä¸‹ä¸€å€‹æ¨¡å‹
                            if '429' in str(result.error):
                                cooldown_until = time.time() + self.cooldown_duration
                                self.model_cooldown[try_model] = cooldown_until
                                print(f"âš ï¸ {try_model} é”åˆ°é™åˆ¶ï¼Œè¨­å®š 2 å°æ™‚å†·å»ï¼Œåˆ‡æ›ä¸‹ä¸€å€‹æ¨¡å‹...")
                                break
                            
                            time.sleep(1)
            
            elif source == 'cloud' and self.cloud_enabled:
                # å˜—è©¦é›²ç«¯æœå‹™
                for attempt in range(retry_count):
                    print(f"â˜ï¸ å˜—è©¦ Ollama Cloud... (ç¬¬ {attempt + 1} æ¬¡)")
                    result = self._generate_cloud(prompt, timeout)
                    
                    if result.success:
                        return result
                    
                    errors.append(f"[cloud] {result.error}")
                    time.sleep(1)
        
        # æ‰€æœ‰æœå‹™éƒ½å¤±æ•—
        return LLMResponse(
            success=False,
            error=f"æ‰€æœ‰æœå‹™éƒ½ç„¡æ³•ä½¿ç”¨ï¼š{'; '.join(errors[-5:])}"  # åªé¡¯ç¤ºæœ€å¾Œ5å€‹éŒ¯èª¤
        )
    
    def generate_stream(
        self, 
        prompt: str,
        model: Optional[str] = None,
        url: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        ä¸²æµç”Ÿæˆæ–‡å­—ï¼ˆåªæ”¯æ´æœ¬åœ°ï¼‰
        
        Args:
            prompt: è¼¸å…¥æç¤ºè©
            model: æŒ‡å®šæ¨¡å‹
            url: æŒ‡å®š URL
            
        Yields:
            ç”Ÿæˆçš„æ–‡å­—ç‰‡æ®µ
        """
        url = url or self.local_primary_url
        model = model or self.local_model
        
        try:
            resp = requests.post(
                f"{url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": True
                },
                stream=True,
                timeout=300
            )
            
            if resp.ok:
                for line in resp.iter_lines():
                    if line:
                        import json
                        data = json.loads(line)
                        if 'response' in data:
                            yield data['response']
        except Exception as e:
            yield f"\n[éŒ¯èª¤ï¼š{e}]"
    
    def get_status(self) -> dict:
        """å–å¾—æ‰€æœ‰æœå‹™çš„é€£æ¥ç‹€æ…‹"""
        status = {
            'local': {
                'primary': {
                    'url': self.local_primary_url,
                    'connected': self.test_connection(self.local_primary_url),
                    'models': []
                },
                'fallback': {
                    'url': self.local_fallback_url,
                    'connected': self.test_connection(self.local_fallback_url),
                    'models': []
                },
                'default_model': self.local_models[0] if self.local_models else 'N/A'
            },
            'cloud': {
                'enabled': self.cloud_enabled,
                'model': self.cloud_model
            }
        }
        
        # å–å¾—å¯ç”¨æ¨¡å‹
        if status['local']['primary']['connected']:
            status['local']['primary']['models'] = self.get_available_models(self.local_primary_url)
        if status['local']['fallback']['connected']:
            status['local']['fallback']['models'] = self.get_available_models(self.local_fallback_url)
        
        return status


# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    config = {
        'local': {
            'primary_url': 'http://192.168.1.100:11434',
            'fallback_url': 'http://localhost:11434',
            'model': 'gemma3:27b'
        },
        'cloud': {
            'enabled': True,
            'url': 'https://api.ollama.com/v1',
            'model': 'deepseek-v3.1:671b-cloud',
            'api_key': ''
        },
        'priority': ['local', 'cloud']
    }
    
    client = OllamaClient(config)
    
    print("ğŸ“Š Ollama æœå‹™ç‹€æ…‹ï¼š")
    status = client.get_status()
    
    print(f"\næœ¬åœ°ä¸»è¦æœå‹™ ({status['local']['primary']['url']}):")
    print(f"  é€£æ¥ï¼š{'âœ…' if status['local']['primary']['connected'] else 'âŒ'}")
    if status['local']['primary']['models']:
        print(f"  æ¨¡å‹ï¼š{', '.join(status['local']['primary']['models'][:5])}")
    
    print(f"\næœ¬åœ°å‚™ç”¨æœå‹™ ({status['local']['fallback']['url']}):")
    print(f"  é€£æ¥ï¼š{'âœ…' if status['local']['fallback']['connected'] else 'âŒ'}")
    
    print(f"\né›²ç«¯æœå‹™ï¼š{'å•Ÿç”¨' if status['cloud']['enabled'] else 'åœç”¨'}")
    print(f"  æ¨¡å‹ï¼š{status['cloud']['model']}")
